{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook adpats the scripts avaible at https://github.com/beir-cellar/beir/wiki/Examples-and-tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, losses, InputExample\n",
    "from beir import util, LoggingHandler\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.train import TrainRetriever\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pathlib, os, gzip, json\n",
    "import logging\n",
    "import random\n",
    "dataset = \"BEIR_dataset_T5AndChatgptMisinfo\"\n",
    "corpus, queries, _ = GenericDataLoader(dataset).load(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#### Parameters for Training ####\n",
    "#################################\n",
    "train_batch_size = 64           \n",
    "max_seq_length = 350            \n",
    "ce_score_margin = 0.1            \n",
    "num_negs_per_system = 32       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_filepath = \"generated/\" + dataset + \"/hard-negatives.jsonl\"\n",
    "train_queries = {}\n",
    "with open(triplets_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in tqdm(fIn, total=502939):\n",
    "#         print(line)\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        #Get the positive passage ids\n",
    "        pos_pids = data['pos'] \n",
    "    \n",
    "        #Get the hard negatives\n",
    "        neg_pids = set()\n",
    "        for system_negs in data['neg'].values():\n",
    "            negs_added = 0\n",
    "            for item in system_negs[5:]:\n",
    "                pid = item #item['pid']\n",
    "                if pid not in neg_pids:\n",
    "                    neg_pids.add(pid)\n",
    "                    negs_added += 1\n",
    "                    if negs_added >= num_negs_per_system:\n",
    "                        break\n",
    "        \n",
    "        if len(pos_pids) > 0 and len(neg_pids) > 0:\n",
    "            train_queries[data['qid']] = {'query': queries[data['qid']], 'pos': pos_pids, 'hard_neg': list(neg_pids)}\n",
    "        \n",
    "print(\"Train queries: {}\".format(len(train_queries)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_queries), len(train_queries['genQ0']['hard_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a custom dataset that returns triplets (query, positive, negative)\n",
    "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, queries, corpus):\n",
    "        self.queries = queries\n",
    "        self.queries_ids = list(queries.keys())\n",
    "        self.corpus = corpus\n",
    "\n",
    "        for qid in self.queries:\n",
    "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
    "            self.queries[qid]['hard_neg'] = list(self.queries[qid]['hard_neg'])\n",
    "            random.shuffle(self.queries[qid]['hard_neg'])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        query = self.queries[self.queries_ids[item]]\n",
    "        query_text = query['query']\n",
    "\n",
    "        pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
    "        pos_text = self.corpus[pos_id][\"text\"]\n",
    "        query['pos'].append(pos_id)\n",
    "\n",
    "        neg_id = query['hard_neg'].pop(0)    #Pop negative and add at end\n",
    "        neg_text = self.corpus[neg_id][\"text\"]\n",
    "        query['hard_neg'].append(neg_id)\n",
    "\n",
    "        return InputExample(texts=[query_text, pos_text, neg_text])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "# We train the SentenceTransformer bi-encoder using MNRL loss\n",
    "device=\"cuda:3\"\n",
    "use_pre_trained_model = False\n",
    "model_name = \"distilroberta-base\"\n",
    "if use_pre_trained_model:\n",
    "    print(\"use pretrained SBERT model\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    model.max_seq_length = max_seq_length\n",
    "else:\n",
    "    print(\"Create new SBERT model\")\n",
    "#     model_name = \"distilbert-base-uncased\" \n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model],device=device)\n",
    "\n",
    "#### Provide a high batch-size to train better with triplets!\n",
    "retriever = TrainRetriever(model=model, batch_size=train_batch_size)\n",
    "\n",
    "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
    "train_dataset = customDataset(train_queries, corpus=corpus)\n",
    "train_dataloader = retriever.prepare_train(train_dataset, shuffle=True, dataset_present=True)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=retriever.model)\n",
    "\n",
    "#### If no dev set is present from above use dummy evaluator\n",
    "ir_evaluator = retriever.load_dummy_evaluator()\n",
    "\n",
    "#### Provide model save path\n",
    "from datetime import datetime\n",
    "model_save_path = os.path.join(\"output\", \"{}-v3-{}\".format(model_name, dataset))\n",
    "# model_save_path = os.path.join(\"output_v2\", \"{}-v3-{}\".format(model_name, dataset), '-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "os.makedirs(model_save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Configure Train params\n",
    "num_epochs = 2\n",
    "evaluation_steps = 10000\n",
    "warmup_steps = 1000\n",
    "\n",
    "retriever.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "                evaluator=ir_evaluator, \n",
    "                epochs=num_epochs,\n",
    "                output_path=model_save_path,\n",
    "                warmup_steps=warmup_steps,\n",
    "                evaluation_steps=evaluation_steps,\n",
    "                optimizer_params = {'lr': 4e-5, 'eps': 1e-6, 'correct_bias': False},\n",
    "                use_amp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyter]",
   "language": "python",
   "name": "conda-env-pyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
