{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset of fact-checks including title and article body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"factchecks.csv\")\n",
    "df = df[['title_body', 'claim']]\n",
    "df.columns = ['source_text', 'target_text']\n",
    "df['source_text'] = \"misinformation: \" + df['source_text']\n",
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model_checkpoint = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = Dataset.from_pandas(train_df)\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "prefix = \"\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"source_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "# model = model.to(\"cuda:1\")\n",
    "\n",
    "batch_size = 12 #32\n",
    "learning_rate = 1e-4\n",
    "num_train_epochs = 5\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-lr_{learning_rate}_{num_train_epochs}ep\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_strategy = 'epoch',\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing BLEU and ROGUE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "x1 = glob.glob('t5-base-finetuned-lr_0.0001_5ep/*') # select model folder here\n",
    "x1 = [it for it in x1 if \"checkpoint\" in it]\n",
    "x2 = []\n",
    "for it in x1:\n",
    "    x2.append([it.split(\"/\")[-1].split(\"-\")[-1], it])\n",
    "epochs = sorted(x2)\n",
    "epochs = [it[1] for it in epochs]\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "import transformers\n",
    "transformers.set_seed(0)\n",
    "\n",
    "filepath = epochs[0]\n",
    "print(filepath)\n",
    "tokenizer = AutoTokenizer.from_pretrained(filepath)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(filepath)\n",
    "model = model.to(\"cuda:0\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.set_seed(0)\n",
    "\n",
    "to_predict = test_df.values\n",
    "true = []\n",
    "pred = []\n",
    "\n",
    "for i in range(len(to_predict)):\n",
    "    text_input = to_predict[i][0]\n",
    "    print(\"Real: \", to_predict[i][1])\n",
    "    input_ids = tokenizer.encode(\n",
    "        text_input, return_tensors=\"pt\", add_special_tokens=True, max_length=512\n",
    "    )\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda:0\"),\n",
    "#         input_ids=input_ids,\n",
    "        top_k = 25,\n",
    "        top_p = 0.95,\n",
    "        do_sample=True,\n",
    "        num_beams = 1,\n",
    "        max_length=64,\n",
    "        num_return_sequences = 1\n",
    "    )\n",
    "    p = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"Predicted: \", p)\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    true.append(to_predict[i][1])\n",
    "    pred.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "bleu1 = []\n",
    "bleu2 = []\n",
    "bleu3 = []\n",
    "bleu4 = []\n",
    "\n",
    "for p, t in zip(pred, true):\n",
    "    reference = [t.lower().split()]\n",
    "    candidate = p.lower().split()\n",
    "    score1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    score2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))\n",
    "    score3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))\n",
    "    score4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    bleu1.append(score1)\n",
    "    bleu2.append(score2)\n",
    "    bleu3.append(score3)\n",
    "    bleu4.append(score4)\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "print(Average(bleu1))\n",
    "print(Average(bleu2))\n",
    "print(Average(bleu3))\n",
    "print(Average(bleu4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(pred, true, avg=True)\n",
    "xx = pd.DataFrame(scores).T['f'].values\n",
    "for x in xx: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing selfBLEU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import copy\n",
    "metric = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "x1 = glob.glob('t5-base-finetuned-lr_0.0001_5ep/*') # select model folder here\n",
    "x1 = [it for it in x1 if \"checkpoint\" in it]\n",
    "x2 = []\n",
    "for it in x1:\n",
    "    x2.append([it.split(\"/\")[-1].split(\"-\")[-1], it])\n",
    "epochs = sorted(x2)\n",
    "epochs = [it[1] for it in epochs]\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "import transformers\n",
    "transformers.set_seed(0)\n",
    "\n",
    "filepath = epochs[0]\n",
    "print(filepath)\n",
    "tokenizer = AutoTokenizer.from_pretrained(filepath)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(filepath)\n",
    "model = model.to(\"cuda:0\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.set_seed(0) \n",
    "\n",
    "to_predict = test_df.values\n",
    "final_score1 = []\n",
    "final_score2 = []\n",
    "final_score3 = []\n",
    "\n",
    "\n",
    "for i in range(len(to_predict)):\n",
    "    print(i)\n",
    "    text_input = to_predict[i][0]\n",
    "    input_ids = tokenizer.encode(\n",
    "        text_input, return_tensors=\"pt\", add_special_tokens=True, max_length=512\n",
    "    )\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda:0\"),\n",
    "        top_k = 25,\n",
    "        top_p = 0.95,\n",
    "        do_sample=True,\n",
    "        num_beams = 1,\n",
    "        max_length=64,\n",
    "        num_return_sequences = 5\n",
    "    )\n",
    "    p = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    def calculate_selfBleu(sentences, order):\n",
    "        def get_bleu_score(sentence, remaining_sentences, order):\n",
    "            lst = []\n",
    "            for i in remaining_sentences:\n",
    "                preds = [sentence.lower().split()]\n",
    "                labels = [[i.lower().split()]]\n",
    "                bleu = metric.compute(predictions=preds, references=labels, max_order=int(order))\n",
    "                lst.append(bleu['bleu'])\n",
    "            return lst\n",
    "        bleu_scores = []\n",
    "        for i in sentences:\n",
    "            sentences_copy = copy.deepcopy(sentences)\n",
    "            remaining_sentences = sentences_copy.remove(i)\n",
    "            bleu = get_bleu_score(i, sentences_copy, order)\n",
    "            bleu_scores.append(bleu)\n",
    "        return np.mean(bleu_scores)\n",
    "    \n",
    "    \n",
    "#     print(p)\n",
    "    final_score1.append(calculate_selfBleu(p, 1))\n",
    "    final_score2.append(calculate_selfBleu(p, 2))\n",
    "    final_score3.append(calculate_selfBleu(p, 3))\n",
    "\n",
    "#     print(final_score1, final_score2, final_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(final_score1)/len(final_score1))\n",
    "print(sum(final_score2)/len(final_score2))\n",
    "print(sum(final_score3)/len(final_score3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "x1 = glob.glob('t5-base-finetuned-lr_0.0001_5ep/*') # select model folder here\n",
    "x1 = [it for it in x1 if \"checkpoint\" in it]\n",
    "x2 = []\n",
    "for it in x1:\n",
    "    x2.append([it.split(\"/\")[-1].split(\"-\")[-1], it])\n",
    "epochs = sorted(x2)\n",
    "epochs = [it[1] for it in epochs]\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "import transformers\n",
    "transformers.set_seed(0)\n",
    "\n",
    "filepath = epochs[0]\n",
    "print(filepath)\n",
    "tokenizer = AutoTokenizer.from_pretrained(filepath)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(filepath)\n",
    "model = model.to(\"cuda:0\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_json = train_df.append(test_df).to_dict(\"records\")\n",
    "for i in range(len(df_json)):\n",
    "    print(i)\n",
    "    t5input = df_json[i]['source_text']\n",
    "    flag = True\n",
    "    tries = 0\n",
    "    while flag:\n",
    "        text_input = t5input\n",
    "        input_ids = tokenizer.encode(\n",
    "            text_input, return_tensors=\"pt\", add_special_tokens=True, max_length=512\n",
    "        )\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids.to(\"cuda:0\"),\n",
    "            top_k = 25,\n",
    "            top_p = 0.95,\n",
    "            do_sample=True,\n",
    "            num_beams = 1,\n",
    "            max_length=64,\n",
    "            num_return_sequences = 30\n",
    "        )\n",
    "        t5output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        temp = [t.lower() for t in t5output]\n",
    "        if len(set(temp)) == 30:\n",
    "            flag = False\n",
    "        else:\n",
    "            print(\"duplicate t5 output\")\n",
    "            flag = True\n",
    "            tries += 1\n",
    "            if tries==1:\n",
    "                print(\"Failed to generate unique\")\n",
    "                break\n",
    "    df_json[i]['generatedT5Misinfo'] = t5output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('T5Misinfo.json', 'w') as fp:\n",
    "    json.dump(df_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyter]",
   "language": "python",
   "name": "conda-env-pyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
